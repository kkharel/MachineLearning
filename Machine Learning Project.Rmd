---
title: "Machine Learning Project"
author: "Kushal Kharel"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The data has been collected from accelerometers in devices like Jawbone Up, Nike FuelBand and Fitbit
which are mounted on the belt, forearm, arm and dumbell of 6 participants that were performing
barbell lifts correctly and incorrectly in 5 different ways.

Our goal of this project is to retrieve the data, explore the data, pre-process the data for standardization/scaling if needed and train a model 
to predict the manner in which participants did the exercise that is; in which of the 5 ways
a barbell lift was performed. The classe variable in the data represents the 5 ways in which
participants lifted barbell

The objective is to understand the manner in which participants did the exercise. For example, to lost weight; we keep track of how much calories we eat, how much we burn ,how much we 
burn by doing different workouts. There are several machines at the gym and we exercise on them. We try to quantify all of this activities but we never try to quantify how well we are 
doing it. How well we use those machines can be quantified using various techniques mentioned below.

The source data for this project comes from source: [Repository](http://groupware.les.inf.puc-rio.br/har)

The training set comes from source: [training](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The testing set comes from source: [testing](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The dataset has been downloaded and saved in a local computer in working directory but we can also directly import the data using url.


```{r loading-packages, echo = FALSE, warning=FALSE, message=FALSE}
library(caret); library(kernlab);library(randomForest);library(dplyr);library(tidyverse);library(Hmisc);library(ggplot2);library(randomForest)
```

Step - 1: Loading the data and taking the peek at the dimension of training and testing set.
```{r reading-data}
traincsv = read.csv("C:\\Users\\kkhar\\OneDrive\\Documents\\Practical Machine Learning\\pml-training.csv")
testcsv = read.csv("C:\\Users\\kkhar\\OneDrive\\Documents\\Practical Machine Learning\\pml-testing.csv")

dim(traincsv); dim(testcsv)
```
Step - 2: Pre-processing the data. This is a very critical step in machine learning because our models learn from the data that gets feed into it which affects the model ability to learn. We need to keep in mind
to handle Null Values, Imputation methods, standardization, how to handle ordinal and nominal factor variables, concept of One-Hot Encoding, Multi-Collinearity and its impact

To handle null values, the simple way is to drop rows and columns that contain null values. It is not always wise to drop all rows and columns that contains null values since it can result in information loss.
This is where the second techniques comes in play.

Imputation is simply the process of substituting null values by using methods like Mean Imputation, Cold deck imputation, Regression imputation etc.

Standardization is the process of transforming/scaling the data such that the mean of values is zero and standard deviation is one. The formula for standardization is given below:

$$
z = \frac {x_i - \mu}{\sigma}
$$
where $x_i$ is the data point and $/mu$ is the mean and $\sigma$ is the standard deviation. The scale function achieves this goal in the model.


When it comes to pre-processing ordinal and nominal categorical variables, we need to treat them differently. Note that R does not use the terms nominal, ordinal and interval/ratio for types of variables.
To transform ordinal categorical variables, we can use the factor function which allows us to assign 
an order to the nominal variables thus making it ordinal variables. We need to set the order of the parameter to TRUE and assigning a vector with the desired level of hierarchy the the argument levels.

One-Hot encoding simply means that we create 'n' columns where n is the number of unique values that the factor variable can take. Note that one-hot encoding results in multi-collinearity issues.

Multi-collinearity occurs when variables are strongly dependent on each other which can impact our model. We won't be able to use the weight vector to calculate the variable importance. To check for 
multi-collinearity, we plot the variables in every possible pairs (corr plot) and see the relationship between them. If they have linear relationship then variables are strongly correlated with each other and thus multi-collinearity issue. To correct this issue, we can simply drop the variables or use techniques like ridge regression or PCA or least squares regression. Simple way is to drop the variables which have VIF greater than 10.


```{r preprocessing-data, message=FALSE, warning=FALSE}
# removing non-numeric variables from training set
traincsv = subset(traincsv, select = -c(X,raw_timestamp_part_1,raw_timestamp_part_2, cvtd_timestamp, new_window, num_window, user_name))

# removing non-numeric variables from testing set
testcsv = subset(testcsv, select = -c(X,raw_timestamp_part_1,raw_timestamp_part_2, cvtd_timestamp, new_window, num_window, user_name))


# creating a vector with all of the column classes from train set

columnClasses <- sapply(traincsv,class)

grep("classe", colnames(traincsv)) # getting the column number of classe variable


# creating a vector with all of the column classes from test set
columnClassestest = sapply(testcsv, class)

# converting all data columns to numeric except classe variable in training set

for(i in 1:152){
  if(columnClasses[i] != "numeric"){
    if(columnClasses[i] == "factor"){
      traincsv <- traincsv[,ifelse(NAcount == 0, TRUE, FALSE)]
      traincsv[traincsv[,i] == "",i] <- NA # if there is an null record then fill it up with NA
      traincsv[,i] <- as.numeric(as.character(traincsv[,i]))
    }
    traincsv[,i] <- as.numeric(traincsv[,i])
  }
}

# converting all data columns to numeric in testing set
for(i in 1:152){
  if(columnClassestest[i] != "numeric"){
    if(columnClassestest[i] == "factor"){
      testcsv <- testcsv[,ifelse(NAcount == 0, TRUE, FALSE)]
      testcsv[testcsv[,i] == "",i] <- NA
      testcsv[,i] <- as.numeric(as.character(testcsv[,i]))
    }
    testcsv[,i] <- as.numeric(testcsv[,i])
  }
}

```

Lets count the number of NA values in training and testing set.
```{r}
# total number of NA values in training set
library(dplyr)
traincsv %>%
  summarise(count = sum(is.na(traincsv)))

# total number of NA values in testing set
testcsv %>%
  summarise(count = sum(is.na(testcsv)))
```

Removing all rows and columns with more than 80% NA in train set

```{r removingNAs}
#newtrain = traincsv[which(rowMeans(!is.na(traincsv)) > 0.80), which(colMeans(!is.na(traincsv)) > 0.80)]
newtrain = traincsv[, which(colMeans(!is.na(traincsv)) > 0.80)]

# remove all columns with more than 80% NA in test set
newtest = testcsv[, which(colMeans(!is.na(testcsv)) > 0.80)]
```


```{r checkingNAs}
# checking if NA's are present in the training dataset
sum(is.na(newtrain))

# checking if NA's are present in the testing dataset
sum(is.na(newtest))
```


Near zero variance function is used below to identify variables that have little or no variance. We do not include them in the model since it adds very little value to the algorithm. These are the variables with very few unique values relative to the number of samples and the ratio of frequency of the most common value to the frequency of the second most common value is large.

```{r nearZeroVar}
#removing nerozerovariance variables from the dataset if any of them exists
nzv = nearZeroVar(newtrain[,-53], freqCut = 95/5, uniqueCut = 10, saveMetrics = TRUE, allowParallel = TRUE)
# freqCut = cutoff for the ratio of the most common value to the second most common value
# uniqueCut = cutoff for the percentage of distinct values out of the number of total samples
nzv

```

Since none of the variables has nzv true, we are including all of the variables in our model.

Step 3: Checking the distribution of the data

Lets plot the histogram of all of the independent variables to see how the data in each of them is being distributed.
```{r DistributionofData}
# looking at the histogram of data
library(Hmisc)

par(mar=c(1,1,1,1))

Hmisc::hist.data.frame(newtrain[-53])
```

Step 4: Partitioning the dataset

```{r partitioningData}

# dividing the training set into validation set, sub-training set and testing set

inBuild = createDataPartition(y = newtrain$classe, p = 0.75, list = FALSE)

validation = newtrain[-inBuild,]; buildData = newtrain[inBuild, ]
inTrain = createDataPartition(y = buildData$classe, p = 0.75, list = FALSE)

training = buildData[inTrain, ]
testing = buildData[-inTrain, ]

```

Step 5: Creating the model, Training the model, Testing the Model and Validating the Model

First Model: Decision Trees:
  Decision Tree creates classification or regression models in tree structure. It breaks down a dataset into smaller subsets with increasing depth of tree with leaf and decision nodes. The        decision node has two or more branches and leaf node represents a classification or decision. The root node corresponds to the top-most node in a tree which is the best predictor. Selecting the   top-most node is out of the scope here. Note: Decision trees can handle categorical and numerical data. 

```{r DecisionTree}
modelFit1 = train(classe~., data = training,
                 preProcess = c("center", "scale"), method = "rpart",
                 trControl = trainControl(method="cv", number=3, verboseIter=F), tuneLength = 4) # setting up control for training to use 3-fold cross validation

# Note: Standardization within the training process.
```



```{r plottingTree}
par(mfrow=c(1,1))

# Plotting decision tree
rpart.plot::rpart.plot(modelFit1$finalModel, cex = 0.5, type = 4, under = TRUE)
```
```{r Prediction}
prediction1 = predict(modelFit1, testing)
cmdecisiontree <- confusionMatrix(prediction1, factor(testing$classe))
cmdecisiontree
```
In the decision tree plot, we start at the root node (depth 0):
  At the top, it is the overall probability of participants that are in A. 28% from classe A are classified as A (this is also a prevalance in statistics summary), 19% from B are classified as    A, 17% from C are classified as A, 16% from D are classified as A and 18% from E are classified as A. The node asks whether roll belt is less than 1.1 or not. If yes, then we go down to the root's     left child node. Further, the node asks if pitch forearm is less than -1.6. If yes, it classifies the classe as A. We keep on going to understand what impacts the likelihood of falling into     one of the given classe.

From the results above, decision tree is not able to accurately classify the classe. The accuracy rate is around 53%, it can change everytime we run the model but should be close to this number.
The model was able to classify classe A pretty well but not others.
We can try to fit a different model and see how it performs. 

Second model: Random Forest: 
  Random Forest is the ensemble of decision trees. It combines multiple decision trees to get more accurate predictions. Random forest chooses the predictors at random and takes the outputs of     multiple trees to make a decision. Think about decision trees and random forests as individual work and group work. 

```{r randomForest}
# random forest
library(randomForest)
modelFit2 = randomForest(as.factor(training$classe)~., data = training, ntree=10, importance = T
                         # preProcess = c("center","scale"),
                         # methods = "rf",
                         # trControl = trainControl(method="cv", number=3, verboseIter=F),  # setting up control for training to use 3-fold cross validation
                                       # importance = TRUE, proximity = TRUE
                                       )
modelFit2
#getTree(modelFit2, k = 4)
```
We can see from training the random forest model, there are 10 trees and number of variables tried at each split is seven. From the confusion matrix, we can see that 3026 were 
correctly identified as A which is indeed A. 77 are classified as A but are B and so on. 

```{r randomForestPlot}
plot(modelFit2)
```

From the model fit plot, we can see that as the number of trees increases, the error decreases

We can also tune the model and make it more accurate by tweaking various hyper parameters.
One simple example of hyperparameter tuning is below.

```{r modeltuning}
# tuning the model

# ntreeTry: The number of trees to build.
# mtryStart: The starting number of predictor variables to consider at each split.
# stepFactor: The factor to increase by until the out-of-bag estimated error stops
# improving by a certain amount.
# improve: The amount that the out-of-bag error needs to improve by to
# keep increasing the step factor

# hyper parameter tuning
tuned_model = tuneRF(
  x= training[,-53], #define predictor variables
  y=as.factor(training$classe), #define response variable
  ntreeTry=500,
  stepFactor=1.5,
  improve=0.01,
)
print(tuned_model)

```

We can see the number of predictors used at each split when building the trees on the x-axis and the out-of-bag estimated error on the y-axis. 
The lowest OOB error is achieved by using 7 randomly chosen predictor at each split when building the trees which is default in the function above.

```{r predictionRF}
prediction2 = predict(modelFit2, testing)
cmrandomforest = confusionMatrix(prediction2, factor(testing$classe))
cmrandomforest
```

From the Confusion Matrix and Statistics summary above, we can see that random forest predicted 1043 records as A which were indeed A. It was able to accurately identify the classe 98.69%.

Variance importance plot helps us identify which predictor variables are important in the final model aka checking for distributional assumptions. The presence of variance in data set is important because it allows the model to learn about different patterns hidden in the data. Also, it must not have high variance because then we will be over-fitting the model. We need a balance between these two. Also refer to bias-variance trade-off to learn more.

```{r VarImpPlot}
varImpPlot(modelFit2,
           type=NULL, class=NULL, scale=TRUE,
           main=deparse(substitute(modelFit2)))

```

The x-axis displays the average increase in node purity of the regression trees based on splitting on the various predictors displayed on the y-axis. From the plot we can see that yaw_belt and roll_belt are the most important predictor variable,


Third Model: Gradient Boosting:
  This is one of the leading methods to win Kaggle competitions. On one hand, random forest builds ensemble of independent trees whereas on the other hand, GBM builds ensemble of shallow and      weak successive trees with each tree learning and improving on the previous. When we combine these weak trees, it can often produce hard to beat predictions than other algorithms can. Gradient   boosting is considered gradient descent algorithm which is an optimization algorithm capable of finding optimal solutions to a given problem. The general idea is to tweak the parameters         iteratively to minimize the cost function. The problem is that not all cost functions are convex, we might get stuck in local minima which can make finding global minima difficult.

```{r GradientBoosting}
# Gradient Boosted Trees

modelFit3 = train(classe~., method = "gbm", data = training,
                preProcess = c("center", "scale"),
                verbose = FALSE) #boosting with trees

plot(modelFit3)
prediction3 = predict(modelFit3, testing)

cmboostingtrees = confusionMatrix(prediction3, as.factor(testing$classe))
cmboostingtrees
```

We can see from the results above, gradient boosting was not a great choice against random forest but it is better than decision trees.The accuracy is 95.68%


Fourth Model: Support Vector Machines: 
  SVM is widely used in classification problems. SVM creates a decision boundary which best separates the given data points. The best hyperplane is the one whose distance to the nearest element   of each tag is the largest. Each data item is plotted as a point in n-dimensional space where n is the number of independent variables with the value of each independent variables being the     value in particular coordinate. SVM can efficiently perform a non-linear classification, by implicitly mapping the inputs into high-dimensional variable spaces.

```{r SVM}
# Support Vector Machine

modelFit4 = train(classe~., method = "svmLinear",
                  data = training,
                  preProcess = c("center", "scale"),
                  trControl = trainControl(method="cv", number=3, verboseIter=F),  # setting up control for training to use 3-fold cross validation
                  tuneLength = 4)

prediction4 = predict(modelFit4, testing)

cmsupportvector = confusionMatrix(prediction4, factor(testing$classe))
cmsupportvector
```

From the results above, SVM performed worse than random forest and gradient boosted trees but still better than decision tree. The accuracy is only 77.98%

Let's combine two worst performing model and create a new model to see whether it performs better than all of the model we have trained so far.

Ensemble Method (combining two methods that have low accuracy, model stacking). In our model,
we are stacking the predictions together using random forests and setting up control for 
training to use 3-fold cross-validation.

```{r EmsembleMethod}
comb = data.frame(prediction1, prediction4, classe = as.factor(testing$classe))
modelComb = randomForest(as.factor(comb$classe)~., data = comb,
                         preProcess = c("center,scale"),
                         trControl = trainControl(method="cv", number=3, verboseIter=F), 
                         importance = TRUE, proximity = TRUE)

getTree(modelComb, k = 4)
predictionComb = predict(modelComb, testing)
cmensemble = confusionMatrix(predictionComb, comb$classe)
cmensemble
```


Let us check the accuracy of each model in the test set.

```{r Accuracy}
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?

confusionMatrix(prediction1, as.factor(testing$classe))$overall[1]
confusionMatrix(prediction2, as.factor(testing$classe))$overall[1]
confusionMatrix(prediction3, as.factor(testing$classe))$overall[1]
confusionMatrix(prediction4, as.factor(testing$classe))$overall[1]
confusionMatrix(predictionComb, as.factor(testing$classe))$overall[1]
```

We can see from the results above that the combined method has much less error than the individual models. However, random forest is still the best performing model among all of them. 

We choose random forest model as the most appropriate model for this data.

Now let's apply the prediction to validation set.
```{r ValidationPred}
# Prediction on validation data set
pred1v = predict(modelFit1, validation);
pred2v = predict(modelFit2, validation);
pred3v = predict(modelFit3, validation);
pred4v = predict(modelFit4, validation);

predvdf = data.frame(prediction1 = pred1v, prediction2 = pred2v,
                     prediction3 = pred3v, prediction4 = pred4v)

combpredv = predict(modelComb,predvdf)
```

Evaluation:

```{r EvaluationPred}
# Evaluate on Validation
cmdecisiontreev = confusionMatrix(pred1v, as.factor(validation$classe))$overall[1]
cmrandomforestv = confusionMatrix(pred2v, as.factor(validation$classe))$overall[1]
cmboostingtreev = confusionMatrix(pred3v, as.factor(validation$classe))$overall[1]
cmsupportvectorv = confusionMatrix(pred4v, as.factor(validation$classe))$overall[1]
cmensemblev = confusionMatrix(combpredv, as.factor(validation$classe))$overall[1]

models <- c("Tree", "RF", "GBM", "SVM", "Ensemble")
accuracy <- round(c( cmdecisiontreev, cmrandomforestv,
                     cmboostingtreev, cmsupportvectorv,
                     cmensemblev),3) #accuracy

oos_error <- 1 - accuracy #out of sample error

data.frame(accuracy = accuracy, oos_error = oos_error, row.names = models)
```

We can see from the table above, Tree has the highest out of sample error followed by SVM and Ensemble. The best model in terms of accuracy and OOS error is random forest followed by GBM .


```{r modelPlots}
# plotting the models

library(ggplot2)
par(mfrow=c(1,1))
ggplot(modelFit1) + theme_bw()
ggplot(modelFit3) + theme_bw()

```

A simple plot between Complexity Parameter vs Accuracy (Cross-Validation) for decision tree model is shown above. As complexity parameter decreases the accuracy of the validation increases.
For gradient boosting ,we can see that bootstrapping accuracy of the model increases as the iterations of boosting increases. We can also see that as the tree depth shifts upwards accuracy also increases.

Finally, let us apply the modelFit2 to the new testing set that does not have classe or some new data we haven't yet seen and predict the classe of each observations.

```{r TestingPred}
# prediction on testing dataset
predc = predict(modelFit2, newtest)
predc

```

