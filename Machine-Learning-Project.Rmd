---
title: "Machine Learning Project"
author: "Kushal Kharel"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The data has been collected from accelerometers in devices like Jawbone Up, Nike FuelBand and Fitbit
which are mounted on the belt, forearm, arm and dumbell of 6 participants that were performing
barbell lifts correctly and incorrectly in 5 different ways.

Our goal of this project is to retrieve the data, pre-process the data and train a model 
to predict the manner in which participants did the exercise that is in which of the 5 ways
a barbell lift was performed. The classe variable in the data represents the 5 ways in which
participants lifted barbell

The source data for this project comes from source: [Repository](http://groupware.les.inf.puc-rio.br/har)

The training set comes from source: [training](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The testing set comes from source: [testing](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

The dataset has been downloaded and saved in a local computer in working directory but
we can also directly import the data using url.


```{r packages, echo = FALSE}
library(caret); library(kernlab);library(randomForest);library(dplyr);library(tidyverse);library(Hmisc);library(ggplot2);library(randomForest)
```

Step - 1: Loading the data
```{r}
traincsv = read.csv("C:\\Users\\kkhar\\OneDrive\\Documents\\Practical Machine Learning\\pml-training.csv")
testcsv = read.csv("C:\\Users\\kkhar\\OneDrive\\Documents\\Practical Machine Learning\\pml-testing.csv")

# looking at the dimension  of the traincsv and testcsv

dim(traincsv); dim(testcsv)
```

Step - 2: Pre-processing the data

```{r}
# removing non-numeric variables from training set
traincsv = subset(traincsv, select = -c(X,raw_timestamp_part_1,raw_timestamp_part_2, cvtd_timestamp, new_window, num_window, user_name))

# removing non-numeric variables from testing set
testcsv = subset(testcsv, select = -c(X,raw_timestamp_part_1,raw_timestamp_part_2, cvtd_timestamp, new_window, num_window, user_name))


# creating a vector with all of the column classes from train set

columnClasses <- sapply(traincsv,class)
columnClasses
grep("classe", colnames(traincsv)) # getting the column number of classe variable


# creating a vector with all of the column classes from test set
columnClassestest = sapply(testcsv, class)
columnClassestest

# converting all data columns to numeric except classe variable in training set

for(i in 1:152){
  if(columnClasses[i] != "numeric"){
    if(columnClasses[i] == "factor"){
      traincsv <- traincsv[,ifelse(NAcount == 0, TRUE, FALSE)]
      traincsv[traincsv[,i] == "",i] <- NA
      traincsv[,i] <- as.numeric(as.character(traincsv[,i]))
    }
    traincsv[,i] <- as.numeric(traincsv[,i])
  }
}

# converting all data columns to numeric from testing set
for(i in 1:152){
  if(columnClassestest[i] != "numeric"){
    if(columnClassestest[i] == "factor"){
      testcsv <- testcsv[,ifelse(NAcount == 0, TRUE, FALSE)]
      testcsv[testcsv[,i] == "",i] <- NA
      testcsv[,i] <- as.numeric(as.character(testcsv[,i]))
    }
    testcsv[,i] <- as.numeric(testcsv[,i])
  }
}

# total number of NA values in training set
library(dplyr)
traincsv %>%
  summarise(count = sum(is.na(traincsv)))

# total number of NA values in testing set
testcsv %>%
  summarise(count = sum(is.na(testcsv)))

```

```{r}
# remove all rows and columns with more than 80% NA in train set
# come back to this later (dropping only columns or rows too)
#newtrain = traincsv[which(rowMeans(!is.na(traincsv)) > 0.80), which(colMeans(!is.na(traincsv)) > 0.80)]
newtrain = traincsv[, which(colMeans(!is.na(traincsv)) > 0.80)]

# remove all columns with more than 80% NA in test set
newtest = testcsv[, which(colMeans(!is.na(testcsv)) > 0.80)]
```

```{r}
as.factor(newtrain$classe)
```


```{r}
# checking if NA's are present in the training dataset
sum(is.na(newtrain))

# checking if NA's are present in the testing dataset
sum(is.na(newtest))
```


```{r}
#removing nerozerovariance variables from the dataset if any of them exists
nzv = nearZeroVar(newtrain[,-53], saveMetrics = TRUE)
nzv

# since none of the variables has nzv true, we are including all of the variables in our model
```

```{r}
# looking at the histogram of data
library(Hmisc)

par(mar=c(1,1,1,1))

Hmisc::hist.data.frame(newtrain[-53])
```

```{r}
# dividing the training set into validation set, sub-training set and testing set

inBuild = createDataPartition(y = newtrain$classe, p = 0.75, list = FALSE)

validation = newtrain[-inBuild,]; buildData = newtrain[inBuild, ]
inTrain = createDataPartition(y = buildData$classe, p = 0.75, list = FALSE)

training = buildData[inTrain, ]
testing = buildData[-inTrain, ]

```

```{r}
# standardizing the data in training
modelFit1 = train(classe~., data = training,
                 preProcess = c("center", "scale"), method = "rpart",
                 trControl = trainControl(method="cv", number=3, verboseIter=F), tuneLength = 4) # setting up control for training to use 3-fold cross validation


```

```{r}
par(mfrow=c(1,1))

# Plotting decision tree
rpart.plot::rpart.plot(modelFit1$finalModel, cex = 0.5, type = 4, under = TRUE)
```
```{r}
prediction1 = predict(modelFit1, testing)
cmdecisiontree <- confusionMatrix(prediction1, factor(testing$classe))
cmdecisiontree
```
From the results above, decision tree is not able to accurately classify the classe. The accuracy rate is around 53%. The model was able to classify classe A pretty well but not others.

We can try to fit a different model and see how it performs. 


```{r}
# random forest
library(randomForest)
modelFit2 = train(classe~., data = training,
                         preProcess = c("center","scale"),
                         methods = "rf",
                         trControl = trainControl(method="cv", number=3, verboseIter=F),  # setting up control for training to use 3-fold cross validation
                                       importance = TRUE, proximity = TRUE
                                       )
modelFit2
#getTree(modelFit2, k = 4)
```
```{r}
plot(modelFit2)
```


Variance importance plot helps us identify which predictor varibales are important in the final model. 

```{r}
varImpPlot(modelFit2,
           type=NULL, class=NULL, scale=TRUE,
           main=deparse(substitute(modelFit2)))

```

The x-axis displays the average increase in node purity of the regression trees
based on splitting on the various predictors displayed on the y-axis. From the plot we can see that yaw_belt is the most important predictor variable,
followed closely by roll_belt.


We can also tune the model and make it more accurate by tweaking various hyper parameters.
One simple example of hyperparameter tuning is below.

```{r}
# tuning the model

# ntreeTry: The number of trees to build.
# mtryStart: The starting number of predictor variables to consider at each split.
# stepFactor: The factor to increase by until the out-of-bag estimated error stops
# improving by a certain amount.
# improve: The amount that the out-of-bag error needs to improve by to
# keep increasing the step factor

# hyper parameter tuning
tuned_model = tuneRF(
  x= training[,-53], #define predictor variables
  y=training$classe, #define response variable
  ntreeTry=500,
  stepFactor=1.5,
  improve=0.01,
)
print(tuned_model)

```

We can see the number of predictors used at each split when building the trees
on the x-axis and the out-of-bag estimated error on the y-axis. The lowest OOB error is achieved by using 7 randomly chosen predictoras at each split when building the trees which is default in the function above.


```{r}
prediction2 = predict(modelFit2, testing)
cmrandomforest = confusionMatrix(prediction2, factor(testing$classe))
cmrandomforest
```


```{r}
# Gradient Boosted Trees

modelFit3 = train(classe~., method = "gbm", data = training,
                preProcess = c("center", "scale"),
                verbose = FALSE) #boosting with trees

plot(modelFit3)
prediction3 = predict(modelFit3, testing)

cmboostingtrees = confusionMatrix(prediction3, testing$classe)
cmboostingtrees
```

```{r}
# Support Vector Machine

modelFit4 = train(classe~., method = "svmLinear",
                  data = training,
                  preProcess = c("center", "scale"),
                  trControl = trainControl(method="cv", number=3, verboseIter=F),  # setting up control for training to use 3-fold cross validation
                  tuneLength = 4)

prediction4 = predict(modelFit4, testing)

cmsupportvector = confusionMatrix(prediction4, factor(testing$classe))
cmsupportvector
```


Ensemble Method (combining two methods that have low accuracy, model stacking). In our model,
we are stacking the predictions together using random forests and setting up control for 
training to use 3-fold cross-validation.

```{r}
comb = data.frame(prediction1, prediction2,prediction3, prediction4, classe = testing$classe)
modelComb = randomForest(classe~., data = comb,
                         preProcess = c("center,scale"),
                         trControl = trainControl(method="cv", number=3, verboseIter=F), 
                         importance = TRUE, proximity = TRUE)

getTree(modelComb, k = 4)
predictionComb = predict(modelComb, testing)
cmensemble = confusionMatrix(predictionComb, comb$classe)
cmensemble
```

```{r}
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?

confusionMatrix(prediction1, testing$classe)$overall[1]
confusionMatrix(prediction2, testing$classe)$overall[1]
confusionMatrix(prediction3, testing$classe)$overall[1]
confusionMatrix(prediction4, testing$classe)$overall[1]
confusionMatrix(predictionComb, testing$classe)$overall[1]

# we can see that combined has much less error
```

```{r}
# Prediction on validation data set
pred1v = predict(modelFit1, validation);
pred2v = predict(modelFit2, validation);
pred3v = predict(modelFit3, validation);
pred4v = predict(modelFit4, validation);

predvdf = data.frame(prediction1 = pred1v, prediction2 = pred2v,
                     prediction3 = pred3v, prediction4 = pred4v)

combpredv = predict(modelComb,predvdf)
```

```{r}
# Evaluate on Validation
cmdecisiontreev = confusionMatrix(pred1v, factor(validation$classe))$overall[1]
cmrandomforestv = confusionMatrix(pred2v, factor(validation$classe))$overall[1]
cmboostingtreev = confusionMatrix(pred3v, factor(validation$classe))$overall[1]
cmsupportvectorv = confusionMatrix(pred4v, factor(validation$classe))$overall[1]
cmensemblev = confusionMatrix(combpredv, validation$classe)$overall[1]

models <- c("Tree", "RF", "GBM", "SVM", "Ensemble")
accuracy <- round(c( cmdecisiontreev, cmrandomforestv,
                     cmboostingtreev, cmsupportvectorv,
                     cmensemblev),3) #accuracy

oos_error <- 1 - accuracy #out of sample error

data.frame(accuracy = accuracy, oos_error = oos_error, row.names = models)
```

```{r}
# plotting the models

library(ggplot2)
par(mfrow=c(1,1))
ggplot(modelFit1) + theme_bw()
plot(modelFit2)
ggplot(modelFit3) + theme_bw()

```

```{r}
# prediction on testing dataset
predc = predict(modelFit2, newtest)
predc


```

